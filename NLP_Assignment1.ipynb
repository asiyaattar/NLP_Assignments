{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHpj-0G9omRQ",
        "outputId": "f39015c5-2f3f-4c45-8008-3a6d5a7e636b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# Install NLTK\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Don't stop learning natural language processing! NLP is amazing ğŸ˜Š #AI @student\"\n",
        "print(\"Original Text:\")\n",
        "print(text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAEj_eMMpQFS",
        "outputId": "2710ca5a-6804-4a29-fe5f-010d216c6ff6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            "Don't stop learning natural language processing! NLP is amazing ğŸ˜Š #AI @student\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "\n",
        "wt = WhitespaceTokenizer()\n",
        "print(\"Whitespace Tokenization:\")\n",
        "print(wt.tokenize(text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gj9Rbd6npSyZ",
        "outputId": "69d3f542-617b-42ae-a6ee-d7a5cd0d658c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Whitespace Tokenization:\n",
            "[\"Don't\", 'stop', 'learning', 'natural', 'language', 'processing!', 'NLP', 'is', 'amazing', 'ğŸ˜Š', '#AI', '@student']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import wordpunct_tokenize\n",
        "\n",
        "print(\"Punctuation Tokenization:\")\n",
        "print(wordpunct_tokenize(text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxRjhLUcpY9Q",
        "outputId": "11bed453-8b42-45cd-8cba-d8de5fe9f0d6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Punctuation Tokenization:\n",
            "['Don', \"'\", 't', 'stop', 'learning', 'natural', 'language', 'processing', '!', 'NLP', 'is', 'amazing', 'ğŸ˜Š', '#', 'AI', '@', 'student']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "tbt = TreebankWordTokenizer()\n",
        "print(\"Treebank Tokenization:\")\n",
        "print(tbt.tokenize(text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YnOQySeplZa",
        "outputId": "bc9513e1-c21b-478a-e28b-f3d4c5eafbb9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Treebank Tokenization:\n",
            "['Do', \"n't\", 'stop', 'learning', 'natural', 'language', 'processing', '!', 'NLP', 'is', 'amazing', 'ğŸ˜Š', '#', 'AI', '@', 'student']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "tweet = TweetTokenizer()\n",
        "print(\"Tweet Tokenization:\")\n",
        "print(tweet.tokenize(text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34K1BWjbprTq",
        "outputId": "68995e4c-9440-412e-85a1-2c8901af50a1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweet Tokenization:\n",
            "[\"Don't\", 'stop', 'learning', 'natural', 'language', 'processing', '!', 'NLP', 'is', 'amazing', 'ğŸ˜Š', '#AI', '@student']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import MWETokenizer\n",
        "\n",
        "mwe = MWETokenizer([('natural', 'language'), ('machine', 'learning')], separator='_')\n",
        "print(\"MWE Tokenization:\")\n",
        "print(mwe.tokenize(text.split()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTp3ow8zpyaM",
        "outputId": "9999087d-a3c6-45b5-d255-d8e244d9c8ac"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MWE Tokenization:\n",
            "[\"Don't\", 'stop', 'learning', 'natural_language', 'processing!', 'NLP', 'is', 'amazing', 'ğŸ˜Š', '#AI', '@student']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "ps = PorterStemmer()\n",
        "words = wordpunct_tokenize(text)\n",
        "\n",
        "print(\"Porter Stemming:\")\n",
        "for word in words:\n",
        "    print(word, \"->\", ps.stem(word))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rzRRV3aqAVR",
        "outputId": "9de0349f-00fe-4452-fabd-bb332405c201"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Porter Stemming:\n",
            "Don -> don\n",
            "' -> '\n",
            "t -> t\n",
            "stop -> stop\n",
            "learning -> learn\n",
            "natural -> natur\n",
            "language -> languag\n",
            "processing -> process\n",
            "! -> !\n",
            "NLP -> nlp\n",
            "is -> is\n",
            "amazing -> amaz\n",
            "ğŸ˜Š -> ğŸ˜Š\n",
            "# -> #\n",
            "AI -> ai\n",
            "@ -> @\n",
            "student -> student\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "ss = SnowballStemmer(\"english\")\n",
        "print(\"Snowball Stemming:\")\n",
        "for word in words:\n",
        "    print(word, \"->\", ss.stem(word))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rz8jFEIgqE3F",
        "outputId": "ab0dc481-5fa8-4443-d21a-caa30d19e8bc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Snowball Stemming:\n",
            "Don -> don\n",
            "' -> '\n",
            "t -> t\n",
            "stop -> stop\n",
            "learning -> learn\n",
            "natural -> natur\n",
            "language -> languag\n",
            "processing -> process\n",
            "! -> !\n",
            "NLP -> nlp\n",
            "is -> is\n",
            "amazing -> amaz\n",
            "ğŸ˜Š -> ğŸ˜Š\n",
            "# -> #\n",
            "AI -> ai\n",
            "@ -> @\n",
            "student -> student\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "print(\"Lemmatization:\")\n",
        "for word in words:\n",
        "    print(word, \"->\", lemmatizer.lemmatize(word))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-WMOzSrqIYr",
        "outputId": "e0f89ba0-d557-49e4-ecc8-1e18a082f3bc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatization:\n",
            "Don -> Don\n",
            "' -> '\n",
            "t -> t\n",
            "stop -> stop\n",
            "learning -> learning\n",
            "natural -> natural\n",
            "language -> language\n",
            "processing -> processing\n",
            "! -> !\n",
            "NLP -> NLP\n",
            "is -> is\n",
            "amazing -> amazing\n",
            "ğŸ˜Š -> ğŸ˜Š\n",
            "# -> #\n",
            "AI -> AI\n",
            "@ -> @\n",
            "student -> student\n"
          ]
        }
      ]
    }
  ]
}